{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dadeedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "#Valence Aware Dictionary and sEntiment Reasoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199b3bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = vader.SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"What a terrible restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aabcab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4767}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf92b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5106}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Even emoticons are supported\n",
    "sia.polarity_scores(\":D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c9483a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.34}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\":/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac48e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.5, 'neu': 0.5, 'pos': 0.0, 'compound': -0.6124}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"the cumin was the kiss of death\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5663fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"the food was good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e085a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.484, 'pos': 0.516, 'compound': 0.4926}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exclamations and other punctuations are also supported:\n",
    "sia.polarity_scores(\"the food was good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f0a35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.428, 'neu': 0.572, 'pos': 0.0, 'compound': -0.457}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"the food was not good!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd2ca599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.245, 'neu': 0.417, 'pos': 0.339, 'compound': 0.2263}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"I usually hate seafood but I like this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "861694b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use Case: Cornell's Movie Review Data\n",
      "\n",
      "For this use case, the following steps are going to be followed:\n",
      "\n",
      "    Download Corpus. The corpus to use is Cornell dataset of 10,000+ pre-classified movie reviews.\n",
      "    Classify with VADER. checking if it is either positive or negative, using VADER's compound score to decide.\n",
      "    Measure accuracy Calculate the percentage of a accuracy.\n",
      "\n",
      "First step is to read the reviews. Both positive and negative reviews are going to be read and stored in different python lists.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fulltext ='''\n",
    "Use Case: Cornell's Movie Review Data\n",
    "\n",
    "For this use case, the following steps are going to be followed:\n",
    "\n",
    "    Download Corpus. The corpus to use is Cornell dataset of 10,000+ pre-classified movie reviews.\n",
    "    Classify with VADER. checking if it is either positive or negative, using VADER's compound score to decide.\n",
    "    Measure accuracy Calculate the percentage of a accuracy.\n",
    "\n",
    "First step is to read the reviews. Both positive and negative reviews are going to be read and stored in different python lists.\n",
    "\n",
    "'''\n",
    "print(fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c012093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string #punctuation\n",
    "import re #url\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #lemmatization\n",
    "from nltk.corpus.reader import NOUN, VERB, ADJ, ADV\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_en = stopwords.words(\"english\")\n",
    "punctuation_translator = str.maketrans(\"\",\"\",string.punctuation)\n",
    "# positive reviews\n",
    "entries = os.listdir('pos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a81cb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_and_remove_messy_text(path, stop_words, punctuation_translator):\n",
    "    file = open(path, 'rb');\n",
    "    content = file.read()\n",
    "    words = word_tokenize(str(content))\n",
    "    \n",
    "    words_without_punctuation = []  \n",
    "    for word in words:\n",
    "        word = word.translate(punctuation_translator)\n",
    "        if len(word)>2:  #don't add empty strings or irrelevant one\n",
    "            words_without_punctuation.append(word)\n",
    "    #print(words_without_punctuation)\n",
    "    \n",
    "    words_without_stop_words_and_punctuation = [word for word in words_without_punctuation if not word in stop_en]\n",
    "    #print(words_without_stop_words_and_punctuation)\n",
    "    \n",
    "    words_without_stop_words_punctuation_and_url = []\n",
    "    for word in words_without_stop_words_and_punctuation:\n",
    "        word = re.sub(r\"http\\S+\", \"\", word)\n",
    "        words_without_stop_words_punctuation_and_url.append(word)\n",
    "    \n",
    "    #print(words_without_stop_words_punctuation_and_url)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "    tag_map['N'] = wordnet.NOUN\n",
    "    tag_map['J'] = wordnet.ADJ\n",
    "    tag_map['V'] = wordnet.VERB\n",
    "    tag_map['R'] = wordnet.ADV\n",
    "    \n",
    "    tags = nltk.pos_tag(words_without_stop_words_punctuation_and_url)\n",
    "    lemmas = [lemmatizer.lemmatize(token,tag_map[tag[0]]) for token,tag in tags]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "506c4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_documents(folder_path, stop_words, punctuation_translator):\n",
    "    file_names = os.listdir(folder_path)\n",
    "    documents = [read_document_and_remove_messy_text(folder_path+'/'+file_name, stop_words, punctuation_translator) \n",
    "                 for file_name in file_names]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47ab003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_docs = clean_all_documents(\"neg/\",stop_en, punctuation_translator)\n",
    "positive_docs = clean_all_documents(\"pos/\",stop_en, punctuation_translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ea9b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents_string_array = []\n",
    "for i in range(len(negative_docs)):\n",
    "    sentence = ' '.join(negative_docs[i])\n",
    "    all_documents_string_array.append(sentence)\n",
    "for i in range(len(positive_docs)):\n",
    "    sentence = ' '.join(positive_docs[i])\n",
    "    all_documents_string_array.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2a64355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bsimply', 'irresistible', 'review', 'jamie', 'peckrating', '20th', 'century', 'fox', '1999', 'pg13', 'language', 'sexual', 'suggestiveness', 'cast', 'sarah', 'michelle', 'gellar', 'sean', 'patrick', 'flanery', 'patricia', 'clarkson', 'dylan', 'baker', 'betty', 'buckley', 'larry', 'gillard', 'director', 'mark', 'tarlov', 'screenplay', 'judith', 'robert', 'buffy', 'find', 'side', 'supernatural', 'spectrum', 'simply', 'irresistible', 'sarah', 'michelle', 'gellar', 'neverceasing', 'crusader', 'neverending', 'evil', 'every', 'tuesday', 'night', 'network', 'play', 'downonherluck', 'chef', 'suspect', 'dabble', 'witchcraft', 'flail', 'restaurant', 'receives', 'help', 'form', 'magically', 'scrumptious', 'meal', 'taste', 'test', 'result', 'vampire', 'slay', 'though', 'place', 'would', 'really', 'liven', 'excruciate', 'movie', 'simply', 'irresistible', 'actually', 'quite', 'easy', 'deny', 'disaster', 'culinary', 'proportion', 'pretty', 'much', 'opening', 'gellar', 'amanda', 'find', 'fall', 'harry', 'henri', 'bendel', 'exec', 'tom', 'sean', 'patrick', 'flannery', 'powder', 'around', 'time', 'discovers', 'uncanny', 'ability', 'mystically', 'manifest', 'emotion', 'cook', 'secret', 'ingredient', 'arouse', 'interest', 'public', 'tiny', 'tribeca', 'eatery', 'love', 'sorcery', 'lot', 'telekinetic', 'crab', 'also', 'figure', 'story', 'tale', 'time', 'play', 'like', 'american', 'spin', '1993', 'mexican', 'classic', 'like', 'water', 'chocolate', 'several', 'difference', 'one', 'like', 'water', 'didnt', 'telekinetic', 'crab', 'two', 'like', 'water', 'good', 'film', 'simply', 'irresistible', 'hand', 'challenge', 'easybake', 'oven', 'amateurishly', 'stag', 'scene', 'cancel', 'badness', 'embarrass', 'seduction', 'literal', 'vanilla', 'fog', 'impromptu', 'dance', 'sequence', 'tom', 'amanda', 'best', 'fred', 'ginger', 'though', 'fred', 'ginger', 'never', 'take', 'part', 'anything', 'garish', 'intrusive', 'musical', 'score', 'distracts', 'flat', 'dialogue', 'regular', 'interval', 'like', 'go', 'miss', 'much', 'fact', 'take', 'away', 'instance', 'salty', 'language', 'madefordisney', 'channel', 'write', 'majority', 'simply', 'irresistible', 'resistibility', 'factor', 'less', 'aforementioned', 'overkill', 'bland', 'lead', 'tom', 'amanda', 'thin', 'ambiguously', 'define', 'fun', 'root', 'inevitable', 'happily', 'ever', 'otherworldly', 'power', 'serve', 'bring', 'together', 'make', 'sense', 'leave', 'hohum', 'relationship', 'base', 'enchant', 'eclair', 'go', 'last', 'long', 'people', 'perhaps', 'sense', 'filmmaker', 'pile', 'weird', 'support', 'character', 'act', 'vet', 'like', 'betty', 'buckley', 'dylan', 'baker', 'patricia', 'clarkson', 'dont', 'anywhere', 'either', 'fact', 'nothing', 'simply', 'irresistible', 'go', 'anywhere', 'save', 'gellar', 'doesnt', 'somewhere', 'much', 'wish', 'somewhere', 'like', 'good', 'movie', 'already', 'prove', 'major', 'talent', 'relatively', 'big', 'smallscreen', 'role', 'sole', 'reason', 'overcook', 'souffle', 'isnt', 'completely', 'fit', 'garbage', 'disposal', 'look', 'smash', 'todd', 'oldhamdesigned', 'duds', 'invest', 'much', 'perfunctory', 'part', 'ever', 'get', 'return', 'certainly', 'dish', 'best', 'serve', 'current', 'condition', 'one', 'guess', 'without', 'gellar', 'simply', 'irresistible', 'would', 'strictly', 'unwatchable', 'xa9', '1999', 'jamie', 'peck', 'email', 'jpeck1', 'umbc', 'edu', 'visit', 'reel', 'deal', 'online', 'http', 'www', 'umbc', 'edujpeck1', 'double', 'pain', 'body', 'wrack', 'great', 'cataclysmic', 'sob', 'two', 'friend', 'weep', 'sympathy', 'three', 'wail', 'gnash', 'great', 'grief', 'mere', 'word', 'encompass', 'sink', 'ultimate', 'form', 'lamentation', 'clean', 'house', 'bitter', 'salt', 'tear', 'course', 'cheek', 'ajax', 'bathtub', 'babo', 'pot', 'pan', 'audience', 'collapse', 'disbelieve', 'laughter', 'roger', 'ebert', 'let', 'talk', 'sex']\n"
     ]
    }
   ],
   "source": [
    "print(negative_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dafbaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'entertain', 'james', 'bond', 'film', 'roger', 'moore', 'era', 'spy', 'love', 'probably', 'big', 'budget', 'bond', 'movie', 'huge', 'set', 'prop', 'well', 'blockbuster', 'action', 'scene', 'like', 'movie', 'star', 'ringo', 'wife', 'barbara', 'bach', 'feature', 'nonstop', 'disco', 'version', 'traditional', 'series', 'score', 'globetrotting', 'action', 'movie', 'bond', 'mr', 'starr', 'beautiful', 'russian', 'spy', 'team', 'end', 'spectre', 'late', 'threat', 'time', 'madman', 'kidnaps', 'two', 'nuclear', 'submarine', 'one', 'russian', 'one', 'american', 'plan', 'destroy', 'civilized', 'world', 'know', 'begin', 'underwater', 'society', 'somebody', 'go', 'little', 'far', 'see', 'little', 'mermaid', 'know', 'shell', 'bra', 'real', 'turnon', 'give', 'break', 'bond', 'formula', 'work', 'especially', 'well', 'even', 'though', 'omnipresent', 'disco', 'score', 'become', 'annoy', 'ski', 'slope', 'chase', 'scene', 'begin', 'blue', 'screen', 'technology', 'bad', 'still', 'enough', 'gadget', 'explosion', 'chase', 'etc', 'make', 'usual', 'bond', 'threewoman', 'quota', 'reach', 'little', 'help', 'mrs', 'starr', 'also', 'make', 'movie', 'standout', 'series', 'beautiful', 'woman', 'hide', 'behind', 'ugly', 'bellbottoms', 'poofy', 'hair', 'make', 'bach', 'straighthaired', 'smartlydressed', 'agent', 'xxx', 'welcome', 'exception', 'say', 'ringo', 'starr', 'lucky', 'bastard', 'earth', 'first', 'talentless', 'self', 'get', 'great', 'rock', 'roll', 'band', 'ever', 'marry', 'one', 'hottest', 'ladies', 'get', 'eat', 'pizza', 'crust', 'first', 'fortunate', 'visit', 'movie', 'critic', 'large', 'website', 'http', 'www', 'missouri', 'educ667778movies', 'html']\n"
     ]
    }
   ],
   "source": [
    "print(positive_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd11302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 700\n",
      "Number of negative reviews: 700\n"
     ]
    }
   ],
   "source": [
    "# Number of positive and negative reviews\n",
    "#The dataset is totally balanced (50% of positive and 50% of negative reviews):\n",
    "print(\"Number of positive reviews: \" + str(len(positive_docs)))\n",
    "print(\"Number of negative reviews: \" + str(len(negative_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acf1987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB #Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.ensemble import RandomForestClassifier  #RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier #GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import RFE #Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98f0c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 5000, binary=True) #play with this value\n",
    "def vectorize_occurences(corpus, vectorizer):\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X\n",
    "\n",
    "def calculate_frequencies(corpus):\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(corpus)\n",
    "    X = tf_transformer.transform(corpus)\n",
    "    return X\n",
    "\n",
    "bag_of_words_occurences = vectorize_occurences(all_documents_string_array, vectorizer)\n",
    "bag_of_words_frequencies = calculate_frequencies(bag_of_words_occurences)\n",
    "#print(bag_of_words_frequencies)    #https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38d5178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents_for_each_class = int(len(negative_docs))\n",
    "negative_labels = np.zeros((1,number_of_documents_for_each_class), dtype=int)[0]\n",
    "positive_labels = np.ones((1,number_of_documents_for_each_class), dtype=int)[0]\n",
    "#1000 0 for negatives and 1000 1 for positives\n",
    "\n",
    "negative_bow = bag_of_words_frequencies[:number_of_documents_for_each_class]\n",
    "positive_bow = bag_of_words_frequencies[number_of_documents_for_each_class:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c281190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold):   #best\n",
    "    clf = RandomForestClassifier(n_estimators = 1000)\n",
    "    #rfe = RFE(estimator=clf, step=0.5)\n",
    "    clf = clf.fit(features_training_set, labels_training_set)     #maybe use rfe instean of clf??\n",
    "    pred = clf.predict(features_test_set)\n",
    "    print(\"Random Forest Classifier Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9d2d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold):  #best\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf = clf.fit(features_training_set, labels_training_set)\n",
    "    pred = clf.predict(features_test_set)\n",
    "    print(\"SVM Classifier Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66df5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_linear_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold): #good\n",
    "    clf = svm.LinearSVC()\n",
    "    clf = clf.fit(features_training_set, labels_training_set)\n",
    "    pred = clf.predict(features_test_set)\n",
    "    print(\"Linear SVM Classifier Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c204c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "738c61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "positivereviews = str(len(positive_docs))\n",
    "negativereviews = str(len(negative_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "510a6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Classifier Accuracy: 0.7785714285714286\n",
      "Average accuracy:  0.7785714285714286\n",
      "Linear SVM Classifier Accuracy: 0.8285714285714286\n",
      "Average accuracy:  0.8035714285714286\n",
      "Linear SVM Classifier Accuracy: 0.7785714285714286\n",
      "Average accuracy:  0.7952380952380952\n",
      "Linear SVM Classifier Accuracy: 0.8142857142857143\n",
      "Average accuracy:  0.8\n",
      "Linear SVM Classifier Accuracy: 0.85\n",
      "Average accuracy:  0.8099999999999999\n",
      "Linear SVM Classifier Accuracy: 0.8285714285714286\n",
      "Average accuracy:  0.813095238095238\n",
      "Linear SVM Classifier Accuracy: 0.8642857142857143\n",
      "Average accuracy:  0.820408163265306\n",
      "Linear SVM Classifier Accuracy: 0.8071428571428572\n",
      "Average accuracy:  0.8187499999999999\n",
      "Linear SVM Classifier Accuracy: 0.8\n",
      "Average accuracy:  0.8166666666666665\n",
      "Linear SVM Classifier Accuracy: 0.85\n",
      "Average accuracy:  0.82\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits = 10)\n",
    "fold = 0; average_accuracy = 0;\n",
    "\n",
    "for train, test in kfold.split(negative_bow):\n",
    "    fold = fold+1\n",
    "    features_training_set = negative_bow[train].toarray().tolist()  # 1800 x 5000  this 5000 depends -> 1250\n",
    "    features_training_set += positive_bow[train].toarray().tolist()\n",
    "\n",
    "    features_test_set = negative_bow[test].toarray().tolist()  # 200 x 5000\n",
    "    features_test_set += positive_bow[test].toarray().tolist()\n",
    "    \n",
    "    labels_training_set = negative_labels[train].tolist()  #1 x 1800\n",
    "    labels_training_set += positive_labels[train].tolist()\n",
    "    \n",
    "    labels_test_set = negative_labels[test].tolist() #1 x 200\n",
    "    labels_test_set += positive_labels[test].tolist()\n",
    "    \n",
    "    #average_accuracy = multinomial_calssifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "    #average_accuracy = random_forest_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "    #average_accuracy = gradient_boosting_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "    #average_accuracy = svm_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "    average_accuracy = svm_linear_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "    #average_accuracy = logistic_regression_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054b0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python394jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
